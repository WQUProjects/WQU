{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5JSeKOMqm/A0/6eBkW9H5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrbundles-GIT/WQU/blob/main/How_to_Pandas_Part_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pandas: Advanced\n",
        "\n",
        "# Calculate Summary Statistics for a DataFrame or Series\n",
        "\n",
        "Many datasets are large, and it can be helpful to get a summary of information in them. Let's load a dataset and examine the first few rows:"
      ],
      "metadata": {
        "id": "JjwmwE9dCcuA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vVCB5LbCUBb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "mexico_city1 = pd.read_csv(\"./data/mexico-city-real-estate-1.csv\")\n",
        "mexico_city1.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mexico_city1.describe()"
      ],
      "metadata": {
        "id": "XJFkBRIeEAUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like most large datasets, this one has many values which are missing. The describe function will ignore missing values in each column. You can also remove rows and columns with missing values, and then get a summary of the data that's still there. We need to remove columns first, before removing the rows; the sequence of operations here is important. The code looks like this:"
      ],
      "metadata": {
        "id": "zl20eiEFDxz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mexico_city1 = mexico_city1.drop(\n",
        "    [\"floor\", \"price_usd_per_m2\", \"expenses\", \"rooms\"], axis=1\n",
        ")\n",
        "mexico_city1 = mexico_city1.dropna(axis=0)\n",
        "mexico_city1.head()"
      ],
      "metadata": {
        "id": "Nq8QbNxLDz45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at our new, cleaner dataset."
      ],
      "metadata": {
        "id": "a-hpeO0gD6Dc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mexico_city1.describe()"
      ],
      "metadata": {
        "id": "QhlA4KCHEOJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practice\n",
        "\n",
        "Reload the mexico-city-real-estate-1.csv dataset. Reverse the sequence of operations by first dropping all rows where there is a missing value, and then dropping the columns, floor, price_usd_per_m2,expenses and rooms. What is the size of the resulting DataFrame?"
      ],
      "metadata": {
        "id": "3zWPFcYIEQpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mexico_city1 = pd.read_csv(\"./data/mexico-city-real-estate-1.csv\")\n",
        "mexico_city1 = ...\n",
        "mexico_city1 = mexico_city1.drop(\n",
        "    [\"floor\", \"price_usd_per_m2\", \"expenses\", \"rooms\"], axis=1\n",
        ")  # REMOVERHS\n",
        "print(mexico_city1.shape)"
      ],
      "metadata": {
        "id": "DUpiyAUyETzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Select a Series from a DataFrame\n",
        "\n",
        "Since the datasets we work with are so large, you might want to focus on a single column of a DataFrame. Let's load up the mexico-city-real-estate-2 dataset, and examine the first few rows to find the column names."
      ],
      "metadata": {
        "id": "QxoeyKNNEVX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mexico_city2 = pd.read_csv(\"./data/mexico-city-real-estate-2.csv\")\n",
        "mexico_city2.head()"
      ],
      "metadata": {
        "id": "6V1GRhRLEYw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maybe we're interested in the surface_covered_in_m2 column. The code to extract just that one column looks like this:"
      ],
      "metadata": {
        "id": "c5JTOSBlEalo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "surface_covered_in_m2 = mexico_city2[\"surface_covered_in_m2\"]\n",
        "surface_covered_in_m2"
      ],
      "metadata": {
        "id": "RkOyOdkIEc1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practice\n",
        "\n",
        "Select the price series from the mexico-city-real-estate-2 dataset, and load it into the mexico_city2 DataFrame"
      ],
      "metadata": {
        "id": "ZclohEpWEeYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "price = ...\n",
        "print(price)"
      ],
      "metadata": {
        "id": "Km48JBgLEgTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Subset a DataFrame by Selecting One or More Columns\n",
        "\n",
        "You may find it more efficient to work with a smaller portion of a dataset that's relevant to you. One way to do this is to select some columns from a DataFrame and make a new DataFrame with them. Let's load a dataset to do this and examine the first few rows to find the column headings:"
      ],
      "metadata": {
        "id": "y9OxHmZYEiXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mexico_city4 = pd.read_csv(\"./data/mexico-city-real-estate-4.csv\")\n",
        "mexico_city4.head()"
      ],
      "metadata": {
        "id": "ytTsJ_XqEmE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's choose \"operation\", \"property_type\", \"place_with_parent_names\", and \"price\":"
      ],
      "metadata": {
        "id": "vD5LJklDEn9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mexico_city4_subset = mexico_city4[\n",
        "    [\"operation\", \"property_type\", \"place_with_parent_names\", \"price\"]\n",
        "]"
      ],
      "metadata": {
        "id": "byVrXlZ_EpmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we've done that, we can find the resulting number of entries in the DataFrame:"
      ],
      "metadata": {
        "id": "OFN7tvsQErS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mexico_city4_subset.shape"
      ],
      "metadata": {
        "id": "eC9qKKIvEsss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practice\n",
        "\n",
        "Load the mexico-city-real-estate-1.csv dataset and subset it to obtain the operation, lat-lon and place_with_property_names columns only. How many entries are in the resulting DataFrame?"
      ],
      "metadata": {
        "id": "HHtN6hVOEvHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mexico_city1 = ...\n",
        "mexico_city1_subset = mexico_city1[\n",
        "    [\"operation\", \"lat-lon\", \"place_with_parent_names\"]\n",
        "]  # REMOVERHS\n",
        "print(mexico_city1_subset.shape)"
      ],
      "metadata": {
        "id": "psYD4sm0ExO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Subset the Columns of a DataFrame Based on Data Types\n",
        "\n",
        "It's helpful to be able to find specific types of entries — typically numeric ones — and put all of these in a separate DataFrame. First, let's take a look at the mexico-city-real-estate-5 dataset."
      ],
      "metadata": {
        "id": "19N6KNyFEy8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mexico_city5 = pd.read_csv(\"./data/mexico-city-real-estate-5.csv\")\n",
        "mexico_city5.head()"
      ],
      "metadata": {
        "id": "ijGnbCP3E2ZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code to subset just the numerical entries looks like this:"
      ],
      "metadata": {
        "id": "GjrrK_DmE4V1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mexico_city5_numbers = mexico_city5.select_dtypes(include=\"number\")\n",
        "mexico_city5_numbers.head()"
      ],
      "metadata": {
        "id": "5v7n1yXfE5oE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practice\n",
        "\n",
        "Create a subset of the DataFrame from mexico-city-real-estate-5 which excludes numbers."
      ],
      "metadata": {
        "id": "qhiBmyElE7SM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mexico_city3 = ...\n",
        "mexico_city3_no_numbers = ...\n",
        "print(mexico_city3_no_numbers.shape)"
      ],
      "metadata": {
        "id": "V2Hfm6cXE-9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working with value_counts in a Series\n",
        "\n",
        "In order to use the data in a series for other types of analysis, it might be helpful to know how often each value occurs in the Series. To do that, we use the value_counts method to aggregate the data. Let's take a look at the number of properties associated with each department in the colombia-real-estate-1 dataset."
      ],
      "metadata": {
        "id": "PbKl6OD5FAnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_csv(\"data/colombia-real-estate-1.csv\", usecols=[\"department\"])\n",
        "df1[\"department\"].value_counts()"
      ],
      "metadata": {
        "id": "VAR4Nwz1FDd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practice\n",
        "\n",
        "Try it yourself! Aggregate the different property types in the colombia-real-estate-2 dataset."
      ],
      "metadata": {
        "id": "VYmPmFajFFQF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I-Psq3XTFIt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Series and Groupby\n",
        "\n",
        "Large Series often include data points that have some attribute in common, but which are nevertheless not grouped together in the dataset. Happily, pandas has a method that will bring these data points together into groups.\n",
        "\n",
        "Let's take a look at the colombia-real-estate-1 dataset. The set includes properties scattered across Colombia, so it might be useful to group properties from the same department together; to do this, we'll use the groupby method. The code looks like this:"
      ],
      "metadata": {
        "id": "CZ3_E9GDFJVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dept_group = df1.groupby(\"department\")"
      ],
      "metadata": {
        "id": "feJ49a8qFMLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make sure we got all the departments in the dataset, let's print the first occurrence of each department."
      ],
      "metadata": {
        "id": "PY3IyErmFOeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dept_group.first()"
      ],
      "metadata": {
        "id": "_XWZGZt0FP-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practice\n",
        "\n",
        "Try it yourself! Group the properties in colombia-real-estate-2 by department, and print the result."
      ],
      "metadata": {
        "id": "9DAqNWIlFRuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = ...\n",
        "dept_group = ...\n",
        "dept_group.first()"
      ],
      "metadata": {
        "id": "Oxk6fVFqFU18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have all the properties grouped by department, we might want to see the properties in just one of the departments. We can use the get_group method to do that. If we just wanted to see the properties in \"Santander\", for example, the code would look like this:"
      ],
      "metadata": {
        "id": "Z4xV-wlRFWTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dept_group1 = df1.groupby(\"department\")\n",
        "dept_group1.get_group(\"Santander\")"
      ],
      "metadata": {
        "id": "DP1ULgrpFYZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also make groups based on more than one category by adding them to the groupby method. After resetting the df1 DataFrame, here's what the code looks like if we want to group properties both by department and by property_type."
      ],
      "metadata": {
        "id": "1gtrzHT9FZv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_csv(\"data/colombia-real-estate-1.csv\")\n",
        "dept_group2 = df1.groupby([\"department\", \"property_type\"])\n",
        "dept_group2.first()"
      ],
      "metadata": {
        "id": "lltZ_uXrFbmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practice\n",
        "\n",
        "Try it yourself! Group the properties in colombia-real-estate-2 by department and property type, and print the result."
      ],
      "metadata": {
        "id": "OsmsQREqFdI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dept_group = ...\n",
        "dept_group.first()"
      ],
      "metadata": {
        "id": "ipWmT6vFFfYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, it's possible to use groupby to calculate aggregations. For example, if we wanted to find the average property area in each department, we would use the .mean() method. This is what the code for that looks like:"
      ],
      "metadata": {
        "id": "rc9tfg3lFhME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dept_group = df1.groupby(\"department\")[\"area_m2\"].mean()\n",
        "dept_group"
      ],
      "metadata": {
        "id": "kxqTz5joFjIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Practice Try it yourself! Use the .mean method in the colombia-real-estate-2 dataset to find the average price in Colombian pesos (\"price_cop\") for properties in each \"department\"."
      ],
      "metadata": {
        "id": "0dlu6nzmFk0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dept_group = ...\n",
        "dept_group"
      ],
      "metadata": {
        "id": "TWOzVdyFFndd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Subset a DataFrame's Columns Based on the Column Data Types\n",
        "\n",
        "It's helpful to be able to find entries of a certain type, typically numerical entries, and put all of these in a separate DataFrame. Let's load a dataset to see how this works:"
      ],
      "metadata": {
        "id": "Cd9xzyq4Fqg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mexico_city5 = pd.read_csv(\"./data/mexico-city-real-estate-5.csv\")\n",
        "mexico_city5.head()"
      ],
      "metadata": {
        "id": "wF9Bbp7AFtZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's get only numerical entries:"
      ],
      "metadata": {
        "id": "Di4-NlNDFvE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mexico_city5_numbers = mexico_city5.select_dtypes(include=\"number\")\n",
        "mexico_city5_numbers.head()"
      ],
      "metadata": {
        "id": "E7lMbYqhFwsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now find all entries which are not numerical entries:"
      ],
      "metadata": {
        "id": "7L98pCEpFyfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mexico_city5_no_numbers = mexico_city5.select_dtypes(exclude=\"number\")\n",
        "mexico_city5_no_numbers.head()"
      ],
      "metadata": {
        "id": "MPzZnX8NF0h1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practice\n",
        "\n",
        "Create a subset of the DataFrame from mexico-city-real-estate-5.csv which excludes numbers. How many entries does it have?"
      ],
      "metadata": {
        "id": "2ba5nolSF2Zs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "87GF9xKSF4gY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Subset a DataFrame's columns based on column names\n",
        "\n",
        "To subset a DataFrame by column names, either define a list of columns to include or define a list of columns to exclude. Once you've done that, you can retain or drop the columns accordingly. For example, let's suppose we want to modify the mexico_city3 dataset and only retain the first three columns. Let's define two lists, one with the columns to retain and one with the columns to drop:"
      ],
      "metadata": {
        "id": "c5cJXFzZF6VM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drop_cols = [\n",
        "    \"lat-lon\",\n",
        "    \"price\",\n",
        "    \"currency\",\n",
        "    \"price_aprox_local_currency\",\n",
        "    \"price_aprox_usd\",\n",
        "    \"surface_total_in_m2\",\n",
        "    \"surface_covered_in_m2\",\n",
        "    \"price_usd_per_m2\",\n",
        "    \"price_per_m2\",\n",
        "    \"floor\",\n",
        "    \"rooms\",\n",
        "    \"expenses\",\n",
        "    \"properati_url\",\n",
        "]\n",
        "\n",
        "keep_cols = [\"operation\", \"property_type\", \"place_with_parent_names\"]"
      ],
      "metadata": {
        "id": "uxpZ2ISUF9_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll explore both approaches to subset mexico_city3. To retain columns based on keep_cols:"
      ],
      "metadata": {
        "id": "ddNqtg8mGAOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mexico_city3_subsetted = mexico_city3[keep_cols]"
      ],
      "metadata": {
        "id": "pMDVLn2IGB8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To drop columns in drop_cols:"
      ],
      "metadata": {
        "id": "j3xg0BJ-GDuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mexico_city3_subsetted = mexico_city3.drop(columns=drop_cols)"
      ],
      "metadata": {
        "id": "HtK1e2QQGFdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practice\n",
        "\n",
        "Create a subset of the DataFrame from mexico-city-real-estate-3.csv which excludes the last two columns."
      ],
      "metadata": {
        "id": "TVJOUKWBGHQV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0h0wn17jGJAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pivot Tables\n",
        "\n",
        "A pivot table allows to aggregate and summarize a DataFrame across multiple variables. For example, let's suppose we wanted to calculate the mean of the price column in the mexico_city3 dataset for the different values in the property_type column:"
      ],
      "metadata": {
        "id": "fX0sSlqGGKAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "mexico_city3_pivot = ...\n",
        "mexico_city3_pivot"
      ],
      "metadata": {
        "id": "pIMa2gy6GM0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Subsetting with Masks\n",
        "\n",
        "Another way to to create subsets from a larger dataset is through masking. Masks are ways to filter out the data you're not interested in so that you can focus on the data you are. For example, we might want to look at properties in Colombia that are bigger than 200 square meters. In order to create this subset, we'll need to use a mask.\n",
        "\n",
        "First, we'll reset our df1 DataFrame so that we can draw on all the data in its original form. Then we'll create a statement and then assign the result to mask."
      ],
      "metadata": {
        "id": "0gFr6cduGOqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_csv(\"data/colombia-real-estate-1.csv\")\n",
        "mask = df1[\"area_m2\"] > 200\n",
        "mask.head()"
      ],
      "metadata": {
        "id": "eZkLrrimGS2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that mask is a Series of Boolean values. Where properties are smaller than 200 square meters, our statement evaluates as False; where they're bigger than 200, it evaluates to True.\n",
        "\n",
        "Once we have our mask, we can use it to select all the rows from df1 that evaluate as True."
      ],
      "metadata": {
        "id": "akOc8FDJGUUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1[mask].head()"
      ],
      "metadata": {
        "id": "BdeGK-PgGV9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practice\n",
        "\n",
        "Try it yourself! Read colombia-real-estate-2 into a DataFrame named df2, and create a mask to select all the properties that are smaller than 100 square meters."
      ],
      "metadata": {
        "id": "o7aa2Pq-GXxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = ...\n",
        "mask = ...\n",
        "df2[mask].head()"
      ],
      "metadata": {
        "id": "C0kAQRKfGaE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also create masks with multiple criteria using & for \"and\" and | for \"or.\" For example, here's how we would find all properties in Atlántico that are bigger than 400 square meters."
      ],
      "metadata": {
        "id": "HnO6SWOJGbZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask = (df1[\"area_m2\"] > 400) & (df1[\"department\"] == \"Atlántico\")\n",
        "df1[mask].head()"
      ],
      "metadata": {
        "id": "sElb3k1RGeZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practice\n",
        "\n",
        "Try it yourself! Create a mask for df2 to select all the properties in Tolima that are smaller than 150 square meters."
      ],
      "metadata": {
        "id": "cH_6fKdpGf8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask = ...\n",
        "df2[mask].head()"
      ],
      "metadata": {
        "id": "XfBvRT0rGiT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reshape a DataFrame based on column values\n",
        "\n",
        "**What's a pivot table?**\n",
        "A pivot table allows you to quickly aggregate and summarize a DataFrame using an aggregation function. For example, to build a pivot table that summarizes the mean of the price_cop column for each of the unique categories in the property_type column in df2:"
      ],
      "metadata": {
        "id": "DJHOYBzkGj0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "pivot1 = pd.pivot_table(df2, values=\"price_cop\", index=\"property_type\", aggfunc=np.mean)\n",
        "pivot1"
      ],
      "metadata": {
        "id": "ixMJSzGmGtaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practice\n",
        "\n",
        "Try it yourself! build a pivot table that summarizes the mean of the price_cop column for each of the unique departments in the department column in df2:"
      ],
      "metadata": {
        "id": "OMycXTUBGvUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REMOVE {\n",
        "pivot2 = pd.pivot_table(df2, values=\"price_cop\", index=\"department\", aggfunc=np.mean)\n",
        "# REMOVE }\n",
        "pivot2"
      ],
      "metadata": {
        "id": "eLq9JGoZGxrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combine multiple categories in a Series\n",
        "\n",
        "Categorical variables can be collapsed into a fewer number of categories. One approach is to retain the values of the most frequently observed values and collapse all remaining values into a single category. For example, to retain only the values of the top 10 most frequent categories in the department column and then collapse the other categories together, use value_counts to generate the count of the values:"
      ],
      "metadata": {
        "id": "V-S_WJ4fG0ma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2[\"department\"].value_counts()"
      ],
      "metadata": {
        "id": "tXOMyZDaG4Cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, select just the top 10 using the head() method, and select the column names by using the index attribute of the series:"
      ],
      "metadata": {
        "id": "iA_nOnxiG5uO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_10 = df2[\"department\"].value_counts().head(10).index"
      ],
      "metadata": {
        "id": "tgt6DKrAG7Kl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, use the apply method and a lambda function to select only the values from the department column and collapse the remaining values into the value Other:"
      ],
      "metadata": {
        "id": "C2jhMcOyG80q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2[\"department\"] = df2[\"department\"].apply(lambda x: x if x in top_10 else \"Other\")"
      ],
      "metadata": {
        "id": "4WQ3XrtbG-YS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practice\n",
        "\n",
        "Try it yourself! Retain the remaining top 5 most frequent values in the department column and collapse the remaining values into the category Other."
      ],
      "metadata": {
        "id": "XibkVbkoHAAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V8fbwJoAHDi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross Tabulation\n",
        "\n",
        "The pandas crosstab function is a useful for working with grouped summary statistics for categorical data. It starts by picking two categorical columns, then defines one as the index and the other as the column. If the aggregate function and value column is not defined, crosstab will simply calculate the frequency of each combination by default. Let's see the example below from the Colombia real estate dataset."
      ],
      "metadata": {
        "id": "MA6JeGG3HETy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"data/colombia-real-estate-1.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "T1Rt4MKcMDDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following function calculates the frequency of each combination for two variables, department and property_type, in the data set."
      ],
      "metadata": {
        "id": "4PndMJfwMEhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.crosstab(index=df[\"department\"], columns=df[\"property_type\"])"
      ],
      "metadata": {
        "id": "x9D2wk2xMGVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the previous example, you can see we have created a DataFrame with the index showing unique observation in variable department, while the columns are the unique observation for the variable property_type. Each cell shows how many data points there are for each combination of department type and property_type. For example, there are 8 apartments in department Antioquia.\n",
        "\n",
        "We can further specify a value column and an aggregate function, like in pivot_table, to conduct more complicated calculations for the two categorical variables. In the following example, we're looking at the average area size for different property types in the departments in Colombia."
      ],
      "metadata": {
        "id": "O9cSpjJtMH8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "pd.crosstab(\n",
        "    index=df[\"department\"],\n",
        "    columns=df[\"property_type\"],\n",
        "    values=df[\"area_m2\"],\n",
        "    aggfunc=np.mean,\n",
        ").round(0)"
      ],
      "metadata": {
        "id": "06iBXXbnMJr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Practice**\n",
        "\n",
        "Create a cross tabulate calculating frequency of combinations from mexico-city-real-estate-3.csv using currency as the index and property_type as the column."
      ],
      "metadata": {
        "id": "we-sIF9ZMMru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read dataset\n",
        "mexico_city3 = ...\n",
        "\n",
        "# Create `crosstab`\n"
      ],
      "metadata": {
        "id": "-8AjMwQRMQGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applying Functions to DataFrames and Series\n",
        "\n",
        "apply is a useful method for to using one function on all the rows or columns of a DataFrame efficiently. Let's take the following real estate dataset as an example:"
      ],
      "metadata": {
        "id": "davQWNQ3MYIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read data, only use the numerical columns\n",
        "df = pd.read_csv(\"data/colombia-real-estate-2.csv\", usecols=[\"area_m2\", \"price_cop\"])\n",
        "df.head()"
      ],
      "metadata": {
        "id": "DWKNohpDMa2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By specifying the function inside apply(), we can transform the whole DataFrame. For example, I am calculating the square root of each row at each column:"
      ],
      "metadata": {
        "id": "dFdIqyAqMdLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "df.apply(np.sqrt)"
      ],
      "metadata": {
        "id": "OwWzM6SbMfby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note you can also specify the \"axis\" inside apply. By default we have axis=0, which means we are applying the function to each column. We can also switch to axis=1 if we want to apply the function to each row. See the following example showing the sum of all rows for each column:"
      ],
      "metadata": {
        "id": "gKimUKqTPGMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.apply(np.sum, axis=0)"
      ],
      "metadata": {
        "id": "MctTwAa3PHIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code will get the sum of all columns for each row:"
      ],
      "metadata": {
        "id": "yOlB9kg5PJF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.apply(np.sum, axis=1)"
      ],
      "metadata": {
        "id": "lvZbTPRmPKeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By specifying the column name, we can also apply the function to a specific column or columns. Note that we can also specify index (row names) to only apply functions to specific rows, however, it is not common in practice."
      ],
      "metadata": {
        "id": "p-LvYj-vPMBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"area_m2\"].apply(np.sqrt)"
      ],
      "metadata": {
        "id": "c8ta46KIPNmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can assign the results to a new column:"
      ],
      "metadata": {
        "id": "nvh8at3NPPE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"area_sqrt\"] = df[\"area_m2\"].apply(np.sqrt)\n",
        "df"
      ],
      "metadata": {
        "id": "niIK_5fqPQhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Practice**\n",
        "\n",
        "Try it yourself! Create a new column named 'sum_columns', which is the sum of all numerical columns in df:"
      ],
      "metadata": {
        "id": "Yxr9LL2BPSC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"sum_columns\"] = ...\n",
        "df.head()"
      ],
      "metadata": {
        "id": "ze92YejePT6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "df.aggregate(), or df.agg(), shares the same concept as df.apply() in terms of applying functions to a DataFrame, but df.aggregate() can only apply aggregate functions like sum, mean, min, max, etc. See the following example for more details:"
      ],
      "metadata": {
        "id": "iDpsMB8oPWFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"data/colombia-real-estate-2.csv\", usecols=[\"area_m2\", \"price_cop\"])\n",
        "df.head()"
      ],
      "metadata": {
        "id": "ShsgZ1HxPXsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can check what's the minimum number for each column calling the min aggregate function:"
      ],
      "metadata": {
        "id": "CadA737IPZT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.agg(\"min\")"
      ],
      "metadata": {
        "id": "mNn9usiGPbhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like apply(), we can also specify the axis argument to switch axis:"
      ],
      "metadata": {
        "id": "YjeFCgx-PZaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.agg(\"min\", axis=1)"
      ],
      "metadata": {
        "id": "GRpBTFOWPeUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can apply aggregate function to a whole DataFrame using df.agg(), or specify the column name for a subset of DataFrame:"
      ],
      "metadata": {
        "id": "-3fVMPS0Pf1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"area_m2\"].agg(\"min\")"
      ],
      "metadata": {
        "id": "UxpSm9FfPhK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also apply a list of aggregate functions to a DataFrame:"
      ],
      "metadata": {
        "id": "ROMfcp7_PiZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.agg([\"sum\", \"max\"])"
      ],
      "metadata": {
        "id": "W6APkpi0Pk1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The syntax above will calculate both sum and max for each column, and store the result as index. Besides, we can also apply different aggregate functions to different columns. In this case, we need to pass a dictionary specifying key as column names, and value as corresponding aggregate function names:"
      ],
      "metadata": {
        "id": "vNUZk3EhPmjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.agg({\"area_m2\": \"sum\", \"price_cop\": \"min\"})"
      ],
      "metadata": {
        "id": "i0CCb5jkPoZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Practice\n",
        "\n",
        "Try it yourself! Find the minimum for column area_m2 and the maximum for column price_cop using df.agg():"
      ],
      "metadata": {
        "id": "hYtl6gs7PqEO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5UtrV0vePr1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References & Further Reading\n",
        "Pandas Documentation on Dropping a Column in a DataFrame\n",
        "Pandas Documentation on Printing out the First Few Rows of a DataFrame\n",
        "Pandas Documentation on Reading in a CSV File Into a DataFrame\n",
        "Getting Started with Pandas Documentation\n",
        "Pandas Documentation on Extracting a Column to a Series\n",
        "Pandas Official Indexing Guide\n",
        "Series in pandas\n",
        "Tutorial for value_counts\n",
        "Tutorial for groupby\n",
        "pandas Documentation for groupby\n",
        "Pandas Official Indexing Guide\n",
        "Online Examples of Selecting Numeric Columns of a DataFrame\n",
        "Official Pandas Documentation on Data Types in a DataFrame\n",
        "Pandas documentation for Boolean indexing\n",
        "More information on creating various kinds of subsets\n",
        "More on boolean indexing\n",
        "A tutorial on using various criteria to create subsets\n",
        "Pandas.DataFrame.apply\n",
        "Pandas.DataFrame.aggregate\n",
        "Copyright © 2022 WorldQuant University. This content is licensed solely for personal use. Redistribution or publication of this material is strictly prohibited."
      ],
      "metadata": {
        "id": "B-KwTdtPPsmy"
      }
    }
  ]
}